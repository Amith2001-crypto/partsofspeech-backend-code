{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python modules -- this cell needs to be run again if you make changes to any of the files\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from helpers import show_model, Dataset\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences in the corpus.\n",
      "There are 45872 sentences in the training set.\n",
      "There are 11468 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n",
    "\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n",
    "\n",
    "assert len(data) == len(data.training_set) + len(data.testing_set), \\\n",
    "       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: b100-39560\n",
      "words:\n",
      "\t('We', 'ran', 'east', 'for', 'about', 'half', 'a', 'mile', 'before', 'we', 'turned', 'back', 'to', 'the', 'road', ',', 'panting', 'from', 'the', 'effort', 'and', 'soaked', 'with', 'sweat', '.')\n",
      "tags:\n",
      "\t('PRON', 'VERB', 'NOUN', 'ADP', 'ADV', 'PRT', 'DET', 'NOUN', 'ADP', 'PRON', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', '.', 'VERB', 'ADP', 'DET', 'NOUN', 'CONJ', 'VERB', 'ADP', 'NOUN', '.')\n"
     ]
    }
   ],
   "source": [
    "key = 'b100-39560'\n",
    "print(\"Sentence: {}\".format(key))\n",
    "print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n",
    "print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1161192 samples of 56057 unique words in the corpus.\n",
      "There are 928458 samples of 50536 unique words in the training set.\n",
      "There are 232734 samples of 25112 unique words in the testing set.\n",
      "There are 5521 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "      .format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\"\n",
    "      .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "      .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab)))\n",
    "\n",
    "assert data.N == data.training_set.N + data.testing_set.N, \\\n",
    "       \"The number of training + test samples should sum to the total number of samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\n",
      "\n",
      "Labels 1: ('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "Sentence 2: ('But', 'there', 'seemed', 'to', 'be', 'some', 'difference', 'of', 'opinion', 'as', 'to', 'how', 'far', 'the', 'board', 'should', 'go', ',', 'and', 'whose', 'advice', 'it', 'should', 'follow', '.')\n",
      "\n",
      "Labels 2: ('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accessing words with Dataset.X and tags with Dataset.Y \n",
    "for i in range(2):    \n",
    "    print(\"Sentence {}:\".format(i + 1), data.X[i])\n",
    "    print()\n",
    "    print(\"Labels {}:\".format(i + 1), data.Y[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t ('Mr.', 'NOUN')\n",
      "\t ('Podger', 'NOUN')\n",
      "\t ('had', 'VERB')\n",
      "\t ('thanked', 'VERB')\n",
      "\t ('him', 'PRON')\n",
      "\t ('gravely', 'ADV')\n",
      "\t (',', '.')\n"
     ]
    }
   ],
   "source": [
    "# use Dataset.stream() (word, tag) samples for the entire corpus\n",
    "print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "for i, pair in enumerate(data.stream()):\n",
    "    print(\"\\t\", pair)\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 12 unique POS which are:\n",
      "['ADV', 'VERB', 'NOUN', 'X', 'CONJ', 'ADJ', 'NUM', 'PRON', 'ADP', '.', 'DET', 'PRT']\n"
     ]
    }
   ],
   "source": [
    "print('there are', len(list(data.training_set.tagset)), 'unique POS which are:')\n",
    "print(list(data.training_set.tagset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 50536 unique words in the train set\n"
     ]
    }
   ],
   "source": [
    "print('there are', len(list(data.training_set.vocab)), 'unique words in the train set') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pair_counts(sequences_A, sequences_B):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the first sequence list\n",
    "    that counts the number of occurrences of the corresponding value from the\n",
    "    second sequences list.\n",
    "    \n",
    "    For example, if sequences_A is tags and sequences_B is the corresponding\n",
    "    words, then if 1244 sequences contain the word \"time\" tagged as a NOUN, then\n",
    "    you should return a dictionary such that pair_counts[NOUN][time] == 1244\n",
    "    \"\"\"\n",
    "    \n",
    "    emission_counts = {}\n",
    "\n",
    "    for sequence_A, sequence_B in zip(sequences_A, sequences_B):\n",
    "        \n",
    "        for a, b in zip(sequence_A, sequence_B):\n",
    "            \n",
    "            if a not in emission_counts.keys():    # Sequence A defines the keys of emission_counts \n",
    "                emission_counts[a] = {}            # emission_counts is a dictionary with values themselves as dictionary\n",
    "            \n",
    "            if b in emission_counts[a].keys():     # Sequence B defines the keys of emission_counts.values (dictionary)\n",
    "                emission_counts[a][b]+=1\n",
    "            else:\n",
    "                emission_counts[a][b]=1\n",
    "    \n",
    "    return emission_counts\n",
    "\n",
    "\n",
    "# Calculate C(t_i, w_i)\n",
    "emission_counts = pair_counts(list(data.training_set.Y), list(data.training_set.X)) # tags sequence x corresponding words sequence, by sentence in train set\n",
    "\n",
    "assert len(emission_counts) == 12, \\\n",
    "       \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert max(emission_counts[\"NOUN\"], key=emission_counts[\"NOUN\"].get) == 'time', \\\n",
    "       \"Hmmm...'time' is expected to be the most common NOUN.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your emission counts look good!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time'] 1275\n"
     ]
    }
   ],
   "source": [
    "max_value = max(emission_counts['NOUN'].values())  # maximum value\n",
    "max_keys = [k for k, v in emission_counts['NOUN'].items() if v == max_value] # getting all keys containing the `maximum`\n",
    "print(max_keys, max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your MFC tagger has all the correct words!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a lookup table mfc_table where mfc_table[word] contains the tag label most frequently assigned to that word\n",
    "from collections import namedtuple\n",
    "\n",
    "FakeState = namedtuple(\"FakeState\", \"name\")\n",
    "\n",
    "class MFCTagger:\n",
    "    # NOTE: You should not need to modify this class or any of its methods\n",
    "    missing = FakeState(name=\"<MISSING>\")\n",
    "    \n",
    "    def __init__(self, table):\n",
    "        self.table = defaultdict(lambda: MFCTagger.missing)\n",
    "        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n",
    "        \n",
    "    def viterbi(self, seq):\n",
    "        \"\"\"This method simplifies predictions by matching the Pomegranate viterbi() interface\"\"\"\n",
    "        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))\n",
    "\n",
    "\n",
    "# TODO: calculate the frequency of each tag being assigned to each word (hint: similar, but not\n",
    "# the same as the emission probabilities) and use it to fill the mfc_table\n",
    "\n",
    "word_counts = pair_counts(list(data.training_set.X), list(data.training_set.Y))\n",
    "\n",
    "mfc_table = {}\n",
    "for word in list(data.training_set.vocab):\n",
    "    mfc_table[word] = max(word_counts[word], key=word_counts[word].get)\n",
    "\n",
    "# DO NOT MODIFY BELOW THIS LINE\n",
    "mfc_model = MFCTagger(mfc_table) # Create a Most Frequent Class tagger instance\n",
    "\n",
    "assert len(mfc_table) == len(data.training_set.vocab), \"\"\n",
    "assert all(k in data.training_set.vocab for k in mfc_table.keys()), \"\"\n",
    "assert sum(int(k not in mfc_table) for k in data.testing_set.vocab) == 5521, \"\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your MFC tagger has all the correct words!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these is the most used POS for a sample of words in the train set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'shortly': 'ADV',\n",
       " 'warranty': 'NOUN',\n",
       " '$10.00': 'NOUN',\n",
       " 'Sympathy': 'NOUN',\n",
       " 'Wilbur': 'NOUN',\n",
       " '1596/7': 'NUM',\n",
       " 'surprised': 'VERB',\n",
       " 'bring': 'VERB',\n",
       " 'softening': 'VERB',\n",
       " 'yodeling': 'VERB'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('these is the most used POS for a sample of words in the train set')\n",
    "{k: mfc_table[k] for k in list(mfc_table)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n",
    "    by the literal string value 'nan'. Pomegranate will ignore these values\n",
    "    during computation.\n",
    "    \"\"\"\n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-23146\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-35462\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', '<MISSING>', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADV', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:3]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, mfc_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\n",
    "    in the input X and comparing the prediction with the true labels in Y.\n",
    "    \n",
    "    The X should be an array whose first dimension is the number of sentences to test,\n",
    "    and each element of the array should be an iterable of the words in the sequence.\n",
    "    The arrays X and Y should have the exact same shape.\n",
    "    \n",
    "    X = [(\"See\", \"Spot\", \"run\"), (\"Run\", \"Spot\", \"run\", \"fast\"), ...]\n",
    "    Y = [(), (), ...]\n",
    "    \"\"\"\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        \n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy mfc_model: 95.72%\n",
      "testing accuracy mfc_model: 93.01%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your MFC tagger accuracy looks correct!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n",
    "print(\"training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\n",
    "\n",
    "mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n",
    "print(\"testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))\n",
    "\n",
    "assert mfc_training_acc >= 0.955, \"Uh oh. Your MFC accuracy on the training set doesn't look right.\"\n",
    "assert mfc_testing_acc >= 0.925, \"Uh oh. Your MFC accuracy on the testing set doesn't look right.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your MFC tagger accuracy looks correct!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your tag unigrams look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequence list that\n",
    "    counts the number of occurrences of the value in the sequences list. The sequences\n",
    "    collection should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the tag NOUN appears 275558 times over all the input sequences,\n",
    "    then you should return a dictionary such that your_unigram_counts[NOUN] == 275558.\n",
    "    \"\"\"\n",
    "    tag_unigrams = {}\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        \n",
    "        for tag in sequence:\n",
    "            \n",
    "            if tag in tag_unigrams.keys():\n",
    "                tag_unigrams[tag]+=1\n",
    "            else:\n",
    "                tag_unigrams[tag]=1\n",
    "                \n",
    "    return tag_unigrams\n",
    "\n",
    "# TODO: call unigram_counts with a list of tag sequences from the training set\n",
    "tag_unigrams = unigram_counts(list(data.training_set.Y))\n",
    "\n",
    "assert set(tag_unigrams.keys()) == data.training_set.tagset, \\\n",
    "       \"Uh oh. It looks like your tag counts doesn't include all the tags!\"\n",
    "assert min(tag_unigrams, key=tag_unigrams.get) == 'X', \\\n",
    "       \"Hmmm...'X' is expected to be the least common class\"\n",
    "assert max(tag_unigrams, key=tag_unigrams.get) == 'NOUN', \\\n",
    "       \"Hmmm...'NOUN' is expected to be the most common class\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your tag unigrams look good!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency of tags in train set:\n",
      "{'.': 117757,\n",
      " 'ADJ': 66754,\n",
      " 'ADP': 115808,\n",
      " 'ADV': 44877,\n",
      " 'CONJ': 30537,\n",
      " 'DET': 109671,\n",
      " 'NOUN': 220632,\n",
      " 'NUM': 11878,\n",
      " 'PRON': 39383,\n",
      " 'PRT': 23906,\n",
      " 'VERB': 146161,\n",
      " 'X': 1094}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "print('frequency of tags in train set:')\n",
    "pp.pprint(tag_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your tag bigrams look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n",
    "    list that counts the number of occurrences of pair in the sequences list. The input\n",
    "    should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n",
    "    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n",
    "    \"\"\"\n",
    "    tag_bigrams = {}\n",
    "\n",
    "    for sequence in sequences:\n",
    "        \n",
    "        l = len(sequence)\n",
    "        \n",
    "        for index in range(l-1):\n",
    "            \n",
    "            bigram = sequence[index:index+2]\n",
    "            \n",
    "            if bigram in tag_bigrams.keys():\n",
    "                tag_bigrams[bigram] += 1\n",
    "            else:\n",
    "                tag_bigrams[bigram] = 1\n",
    "                \n",
    "    return tag_bigrams     \n",
    "\n",
    "# TODO: call bigram_counts with a list of tag sequences from the training set\n",
    "tag_bigrams = bigram_counts(list(data.training_set.Y))\n",
    "\n",
    "assert len(tag_bigrams) == 144, \\\n",
    "       \"Uh oh. There should be 144 pairs of bigrams (12 tags x 12 tags)\"\n",
    "assert min(tag_bigrams, key=tag_bigrams.get) in [('X', 'NUM'), ('PRON', 'X')], \\\n",
    "       \"Hmmm...The least common bigram should be one of ('X', 'NUM') or ('PRON', 'X').\"\n",
    "assert max(tag_bigrams, key=tag_bigrams.get) in [('DET', 'NOUN')], \\\n",
    "       \"Hmmm...('DET', 'NOUN') is expected to be the most common bigram.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your tag bigrams look good!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency of tags in train set:\n",
      "{('.', '.'): 12588,\n",
      " ('.', 'ADJ'): 3334,\n",
      " ('.', 'ADP'): 7595,\n",
      " ('.', 'ADV'): 5124,\n",
      " ('.', 'CONJ'): 8174,\n",
      " ('.', 'DET'): 8008,\n",
      " ('.', 'NOUN'): 9782,\n",
      " ('.', 'NUM'): 1412,\n",
      " ('.', 'PRON'): 5448,\n",
      " ('.', 'PRT'): 2168,\n",
      " ('.', 'VERB'): 9041,\n",
      " ('.', 'X'): 147,\n",
      " ('ADJ', '.'): 6666,\n",
      " ('ADJ', 'ADJ'): 3758,\n",
      " ('ADJ', 'ADP'): 5895,\n",
      " ('ADJ', 'ADV'): 645,\n",
      " ('ADJ', 'CONJ'): 2500,\n",
      " ('ADJ', 'DET'): 386,\n",
      " ('ADJ', 'NOUN'): 43664,\n",
      " ('ADJ', 'NUM'): 467,\n",
      " ('ADJ', 'PRON'): 249,\n",
      " ('ADJ', 'PRT'): 1301,\n",
      " ('ADJ', 'VERB'): 1167,\n",
      " ('ADJ', 'X'): 31,\n",
      " ('ADP', '.'): 1099,\n",
      " ('ADP', 'ADJ'): 9533,\n",
      " ('ADP', 'ADP'): 2347,\n",
      " ('ADP', 'ADV'): 1805,\n",
      " ('ADP', 'CONJ'): 217,\n",
      " ('ADP', 'DET'): 52841,\n",
      " ('ADP', 'NOUN'): 29965,\n",
      " ('ADP', 'NUM'): 3467,\n",
      " ('ADP', 'PRON'): 8109,\n",
      " ('ADP', 'PRT'): 1675,\n",
      " ('ADP', 'VERB'): 4690,\n",
      " ('ADP', 'X'): 53,\n",
      " ('ADV', '.'): 7577,\n",
      " ('ADV', 'ADJ'): 6143,\n",
      " ('ADV', 'ADP'): 6352,\n",
      " ('ADV', 'ADV'): 4336,\n",
      " ('ADV', 'CONJ'): 789,\n",
      " ('ADV', 'DET'): 3309,\n",
      " ('ADV', 'NOUN'): 1478,\n",
      " ('ADV', 'NUM'): 597,\n",
      " ('ADV', 'PRON'): 2136,\n",
      " ('ADV', 'PRT'): 1305,\n",
      " ('ADV', 'VERB'): 10835,\n",
      " ('ADV', 'X'): 4,\n",
      " ('CONJ', '.'): 612,\n",
      " ('CONJ', 'ADJ'): 3372,\n",
      " ('CONJ', 'ADP'): 2222,\n",
      " ('CONJ', 'ADV'): 2759,\n",
      " ('CONJ', 'CONJ'): 9,\n",
      " ('CONJ', 'DET'): 4636,\n",
      " ('CONJ', 'NOUN'): 7502,\n",
      " ('CONJ', 'NUM'): 575,\n",
      " ('CONJ', 'PRON'): 2058,\n",
      " ('CONJ', 'PRT'): 760,\n",
      " ('CONJ', 'VERB'): 6012,\n",
      " ('CONJ', 'X'): 18,\n",
      " ('DET', '.'): 1385,\n",
      " ('DET', 'ADJ'): 26236,\n",
      " ('DET', 'ADP'): 978,\n",
      " ('DET', 'ADV'): 1937,\n",
      " ('DET', 'CONJ'): 70,\n",
      " ('DET', 'DET'): 662,\n",
      " ('DET', 'NOUN'): 68785,\n",
      " ('DET', 'NUM'): 1073,\n",
      " ('DET', 'PRON'): 1093,\n",
      " ('DET', 'PRT'): 220,\n",
      " ('DET', 'VERB'): 7062,\n",
      " ('DET', 'X'): 156,\n",
      " ('NOUN', '.'): 62639,\n",
      " ('NOUN', 'ADJ'): 2839,\n",
      " ('NOUN', 'ADP'): 53884,\n",
      " ('NOUN', 'ADV'): 5804,\n",
      " ('NOUN', 'CONJ'): 13185,\n",
      " ('NOUN', 'DET'): 3425,\n",
      " ('NOUN', 'NOUN'): 32990,\n",
      " ('NOUN', 'NUM'): 1783,\n",
      " ('NOUN', 'PRON'): 4369,\n",
      " ('NOUN', 'PRT'): 3946,\n",
      " ('NOUN', 'VERB'): 34972,\n",
      " ('NOUN', 'X'): 74,\n",
      " ('NUM', '.'): 3210,\n",
      " ('NUM', 'ADJ'): 705,\n",
      " ('NUM', 'ADP'): 1559,\n",
      " ('NUM', 'ADV'): 234,\n",
      " ('NUM', 'CONJ'): 442,\n",
      " ('NUM', 'DET'): 156,\n",
      " ('NUM', 'NOUN'): 4524,\n",
      " ('NUM', 'NUM'): 267,\n",
      " ('NUM', 'PRON'): 109,\n",
      " ('NUM', 'PRT'): 69,\n",
      " ('NUM', 'VERB'): 537,\n",
      " ('NUM', 'X'): 3,\n",
      " ('PRON', '.'): 4078,\n",
      " ('PRON', 'ADJ'): 359,\n",
      " ('PRON', 'ADP'): 2216,\n",
      " ('PRON', 'ADV'): 2099,\n",
      " ('PRON', 'CONJ'): 455,\n",
      " ('PRON', 'DET'): 695,\n",
      " ('PRON', 'NOUN'): 340,\n",
      " ('PRON', 'NUM'): 39,\n",
      " ('PRON', 'PRON'): 318,\n",
      " ('PRON', 'PRT'): 919,\n",
      " ('PRON', 'VERB'): 27860,\n",
      " ('PRON', 'X'): 1,\n",
      " ('PRT', '.'): 1794,\n",
      " ('PRT', 'ADJ'): 467,\n",
      " ('PRT', 'ADP'): 2189,\n",
      " ('PRT', 'ADV'): 866,\n",
      " ('PRT', 'CONJ'): 285,\n",
      " ('PRT', 'DET'): 2021,\n",
      " ('PRT', 'NOUN'): 845,\n",
      " ('PRT', 'NUM'): 117,\n",
      " ('PRT', 'PRON'): 166,\n",
      " ('PRT', 'PRT'): 261,\n",
      " ('PRT', 'VERB'): 14886,\n",
      " ('PRT', 'X'): 2,\n",
      " ('VERB', '.'): 11699,\n",
      " ('VERB', 'ADJ'): 8423,\n",
      " ('VERB', 'ADP'): 24927,\n",
      " ('VERB', 'ADV'): 15076,\n",
      " ('VERB', 'CONJ'): 2105,\n",
      " ('VERB', 'DET'): 23764,\n",
      " ('VERB', 'NOUN'): 14230,\n",
      " ('VERB', 'NUM'): 1320,\n",
      " ('VERB', 'PRON'): 8001,\n",
      " ('VERB', 'PRT'): 9556,\n",
      " ('VERB', 'VERB'): 26957,\n",
      " ('VERB', 'X'): 28,\n",
      " ('X', '.'): 303,\n",
      " ('X', 'ADJ'): 3,\n",
      " ('X', 'ADP'): 61,\n",
      " ('X', 'ADV'): 7,\n",
      " ('X', 'CONJ'): 24,\n",
      " ('X', 'DET'): 5,\n",
      " ('X', 'NOUN'): 58,\n",
      " ('X', 'NUM'): 1,\n",
      " ('X', 'PRON'): 9,\n",
      " ('X', 'PRT'): 8,\n",
      " ('X', 'VERB'): 62,\n",
      " ('X', 'X'): 552}\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "print('frequency of tags in train set:')\n",
    "pp.pprint(tag_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your starting tag counts look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def starting_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the beginning of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 8093 sequences start with NOUN, then you should return a\n",
    "    dictionary such that your_starting_counts[NOUN] == 8093\n",
    "    \"\"\"\n",
    "    tag_starts = {}\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        if sequence[0] in tag_starts.keys():\n",
    "            tag_starts[sequence[0]]+=1\n",
    "        else:\n",
    "            tag_starts[sequence[0]]=1\n",
    "\n",
    "    return tag_starts\n",
    "\n",
    "# TODO: Calculate the count of each tag starting a sequence\n",
    "tag_starts = starting_counts(list(data.training_set.Y))\n",
    "\n",
    "assert len(tag_starts) == 12, \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert min(tag_starts, key=tag_starts.get) == 'X', \"Hmmm...'X' is expected to be the least common starting bigram.\"\n",
    "assert max(tag_starts, key=tag_starts.get) == 'DET', \"Hmmm...'DET' is expected to be the most common starting bigram.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your starting tag counts look good!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency of starting tags in train set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ADV': 4185,\n",
       " 'ADP': 5583,\n",
       " 'ADJ': 1582,\n",
       " 'PRT': 1718,\n",
       " 'DET': 9763,\n",
       " 'PRON': 7318,\n",
       " 'NOUN': 6469,\n",
       " 'CONJ': 2282,\n",
       " '.': 4107,\n",
       " 'NUM': 760,\n",
       " 'VERB': 2080,\n",
       " 'X': 25}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('frequency of starting tags in train set:')\n",
    "tag_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your ending tag counts look good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ending_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the end of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 18 sequences end with DET, then you should return a\n",
    "    dictionary such that your_ending_counts[DET] == 18\n",
    "    \"\"\"\n",
    "    tag_ends = {}\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        \n",
    "        if sequence[-1] in tag_ends.keys():\n",
    "            tag_ends[sequence[-1]]+=1\n",
    "        else:\n",
    "            tag_ends[sequence[-1]]=1\n",
    "    \n",
    "    return tag_ends\n",
    "\n",
    "# TODO: Calculate the count of each tag ending a sequence\n",
    "tag_ends = ending_counts(list(data.training_set.Y))\n",
    "\n",
    "assert len(tag_ends) == 12, \"Uh oh. There should be 12 tags in your dictionary.\"\n",
    "assert min(tag_ends, key=tag_ends.get) in ['X', 'CONJ'], \"Hmmm...'X' or 'CONJ' should be the least common ending bigram.\"\n",
    "assert max(tag_ends, key=tag_ends.get) == '.', \"Hmmm...'.' is expected to be the most common ending bigram.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your ending tag counts look good!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency of ending tags in train set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': 44936,\n",
       " 'NOUN': 722,\n",
       " 'NUM': 63,\n",
       " 'VERB': 75,\n",
       " 'ADJ': 25,\n",
       " 'ADV': 16,\n",
       " 'ADP': 7,\n",
       " 'DET': 14,\n",
       " 'CONJ': 2,\n",
       " 'PRON': 4,\n",
       " 'PRT': 7,\n",
       " 'X': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('frequency of ending tags in train set:')\n",
    "tag_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your HMM network topology looks good!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "import pandas as pd\n",
    "# TODO: create states with emission probability distributions P(word | tag) and add to the model\n",
    "# (Hint: you may need to loop & create/add new states)\n",
    "\n",
    "states = {}\n",
    "\n",
    "for tag in list(data.tagset):\n",
    "    \n",
    "    state_emissions = DiscreteDistribution( {word: occurence / tag_unigrams[tag] for word, \n",
    "                                             occurence in emission_counts[tag].items()} )\n",
    "    \n",
    "    states[tag] = State(state_emissions, name = tag)\n",
    "    \n",
    "    # transition edge from start to each tag\n",
    "    # For the start tag, how likely is it to be tag_i ? distribution of probabilities for <start> over tags which adds up to 1.\n",
    "    basic_model.add_transition(basic_model.start, states[tag], tag_starts[tag] / len(data.training_set))\n",
    "    \n",
    "    # transition edge from a tag to the end\n",
    "    # for a tag_i, how likely it is at the end of the sentence or not\n",
    "    basic_model.add_transition(states[tag], basic_model.end, tag_ends[tag] / tag_ends[tag])\n",
    "    \n",
    "basic_model.add_states(*states.values())\n",
    "    \n",
    "# TODO: add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n",
    "# (Hint: you may need to loop & add transitions\n",
    "\n",
    "import itertools\n",
    "\n",
    "for tag_1, tag_2 in itertools.product(data.training_set.tagset, data.training_set.tagset):\n",
    "\n",
    "    basic_model.add_transition(states[tag_1], states[tag_2], tag_bigrams[(tag_1, tag_2)] / tag_unigrams[tag_1])\n",
    "\n",
    "# NOTE: YOU SHOULD NOT NEED TO MODIFY ANYTHING BELOW THIS LINE\n",
    "# finalize the model\n",
    "basic_model.bake()\n",
    "\n",
    "assert all(tag in set(s.name for s in basic_model.states) for tag in data.training_set.tagset), \\\n",
    "       \"Every state in your network should use the name of the associated tag, which must be one of the training set tags.\"\n",
    "assert basic_model.edge_count() == 168, \\\n",
    "       (\"Your network should have an edge from the start node to each state, one edge between every \" +\n",
    "        \"pair of tags (states), and an edge from each state to the end node.\")\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your HMM network topology looks good!</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy basic hmm model: 97.54%\n",
      "testing accuracy basic hmm model: 95.94%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"alert alert-block alert-success\">Your HMM tagger accuracy looks correct! Congratulations, you've finished the project.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)\n",
    "print(\"training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)\n",
    "print(\"testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))\n",
    "\n",
    "assert hmm_training_acc > 0.97, \"Uh oh. Your HMM accuracy on the training set doesn't look right.\"\n",
    "assert hmm_testing_acc > 0.955, \"Uh oh. Your HMM accuracy on the testing set doesn't look right.\"\n",
    "HTML('<div class=\"alert alert-block alert-success\">Your HMM tagger accuracy looks correct! Congratulations, you\\'ve finished the project.</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: b100-28144\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('CONJ', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'NOUN', 'NUM', '.', 'CONJ', 'NOUN', 'NUM', '.', '.', 'NOUN', '.', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-23146\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', '.', 'ADP', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n",
      "\n",
      "\n",
      "Sentence Key: b100-35462\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '.', 'ADP', 'ADJ', 'NOUN', '.', 'CONJ', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', '.', 'ADJ', '.', 'CONJ', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:3]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, basic_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] \"dot\" not found in path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydot.py:1923\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1923\u001b[0m     stdout_data, stderr_data, process \u001b[38;5;241m=\u001b[39m \u001b[43mcall_graphviz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m        \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1928\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydot.py:132\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[1;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m program_with_args \u001b[38;5;241m=\u001b[39m [program, ] \u001b[38;5;241m+\u001b[39m arguments\n\u001b[1;32m--> 132\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[0;32m    133\u001b[0m     program_with_args,\n\u001b[0;32m    134\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    135\u001b[0m     cwd\u001b[38;5;241m=\u001b[39mworking_dir,\n\u001b[0;32m    136\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    137\u001b[0m     stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    138\u001b[0m     stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    141\u001b[0m stdout_data, stderr_data \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\subprocess.py:1420\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mshow_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexampleHMM.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_ends\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Part-Of-Speech-tagging-NLP-task-main\\Part-Of-Speech-tagging-NLP-task-main\\helpers.py:95\u001b[0m, in \u001b[0;36mshow_model\u001b[1;34m(model, figsize, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"Display a Pomegranate model as an image using matplotlib\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m    for details\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39mfigsize)\n\u001b[1;32m---> 95\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(model2png(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m     96\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Part-Of-Speech-tagging-NLP-task-main\\Part-Of-Speech-tagging-NLP-task-main\\helpers.py:64\u001b[0m, in \u001b[0;36mmodel2png\u001b[1;34m(model, filename, overwrite, show_ends)\u001b[0m\n\u001b[0;32m     62\u001b[0m pydot_graph \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mdrawing\u001b[38;5;241m.\u001b[39mnx_pydot\u001b[38;5;241m.\u001b[39mto_pydot(g)\n\u001b[0;32m     63\u001b[0m pydot_graph\u001b[38;5;241m.\u001b[39mset_rankdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m png_data \u001b[38;5;241m=\u001b[39m \u001b[43mpydot_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m img_data \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[0;32m     66\u001b[0m img_data\u001b[38;5;241m.\u001b[39mwrite(png_data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydot.py:1733\u001b[0m, in \u001b[0;36mDot.__init__.<locals>.new_method\u001b[1;34m(f, prog, encoding)\u001b[0m\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_method\u001b[39m(\n\u001b[0;32m   1730\u001b[0m         f\u001b[38;5;241m=\u001b[39mfrmt, prog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog,\n\u001b[0;32m   1731\u001b[0m         encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;124;03m\"\"\"Refer to docstring of method `create`.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydot.py:1933\u001b[0m, in \u001b[0;36mDot.create\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1930\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(e\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m   1931\u001b[0m     args[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{prog}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found in path.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1932\u001b[0m         prog\u001b[38;5;241m=\u001b[39mprog)\n\u001b[1;32m-> 1933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] \"dot\" not found in path."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_model(basic_model, figsize=(25, 25), filename=\"exampleHMM.png\", overwrite=True, show_ends=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.60516956e-02 1.74941494e-02 3.98524490e-02 2.68866292e-02\n",
      "  4.28905751e-02 4.20195407e-02 5.13280653e-02 7.40903989e-03\n",
      "  2.85867205e-02 1.13759196e-02 4.74398935e-02 7.71337722e-04\n",
      "  0.00000000e+00 6.17893987e-01]\n",
      " [4.99389436e-02 2.81533978e-02 4.41629271e-02 4.83207600e-03\n",
      "  1.87289767e-02 2.89175401e-03 3.27112816e-01 3.49857285e-03\n",
      "  1.86540608e-03 9.74655949e-03 8.74268634e-03 2.32239311e-04\n",
      "  0.00000000e+00 5.00093645e-01]\n",
      " [4.74506603e-03 4.11598857e-02 1.01334577e-02 7.79330680e-03\n",
      "  9.36923865e-04 2.28147438e-01 1.29377528e-01 1.49691937e-02\n",
      "  3.50115927e-02 7.23201601e-03 2.02496448e-02 2.28833939e-04\n",
      "  0.00000000e+00 5.00015110e-01]\n",
      " [8.44346876e-02 6.84548352e-02 7.07838374e-02 4.83184381e-02\n",
      "  8.79226192e-03 3.68740110e-02 1.64701687e-02 6.65270008e-03\n",
      "  2.38026254e-02 1.45423344e-02 1.20740378e-01 4.45742049e-05\n",
      "  0.00000000e+00 5.00089148e-01]\n",
      " [1.00209588e-02 5.52135184e-02 3.63832852e-02 4.51761854e-02\n",
      "  1.47367042e-04 7.59104007e-02 1.22838616e-01 9.41511656e-03\n",
      "  3.36979302e-02 1.24443280e-02 9.84411840e-02 2.94734084e-04\n",
      "  0.00000000e+00 5.00016373e-01]\n",
      " [6.31474320e-03 1.19619930e-01 4.45907498e-03 8.83152172e-03\n",
      "  3.19156696e-04 3.01831047e-03 3.13617048e-01 4.89221621e-03\n",
      "  4.98340384e-03 1.00306390e-03 3.21983513e-02 7.11263494e-04\n",
      "  0.00000000e+00 5.00031915e-01]\n",
      " [1.42186216e-01 6.44433449e-03 1.22312969e-01 1.31746803e-02\n",
      "  2.99290420e-02 7.77451413e-03 7.48850281e-02 4.04728721e-03\n",
      "  9.91732912e-03 8.95714825e-03 7.93840316e-02 1.67974904e-04\n",
      "  0.00000000e+00 5.00819446e-01]\n",
      " [1.35483054e-01 2.97556240e-02 6.58000252e-02 9.87633477e-03\n",
      "  1.86552990e-02 6.58422318e-03 1.90942472e-01 1.12691512e-02\n",
      "  4.60051491e-03 2.91225256e-03 2.26649221e-02 1.26619676e-04\n",
      "  0.00000000e+00 5.01329506e-01]\n",
      " [5.17762374e-02 4.55803561e-03 2.81353953e-02 2.66499074e-02\n",
      "  5.77689750e-03 8.82405222e-03 4.31680253e-03 4.95162643e-04\n",
      "  4.03748001e-03 1.16680633e-02 3.53723878e-01 1.26964780e-05\n",
      "  0.00000000e+00 5.00025394e-01]\n",
      " [3.75274552e-02 9.76885261e-03 4.57901892e-02 1.81152599e-02\n",
      "  5.96171947e-03 4.22759125e-02 1.76759753e-02 2.44744273e-03\n",
      "  3.47244012e-03 5.45967994e-03 3.11390021e-01 4.18366279e-05\n",
      "  0.00000000e+00 5.00073213e-01]\n",
      " [4.00312064e-02 2.88215105e-02 8.52942886e-02 5.15865004e-02\n",
      "  7.20281131e-03 8.13147781e-02 4.86916888e-02 4.51672728e-03\n",
      "  2.73775265e-02 3.26983681e-02 9.22404677e-02 9.58093666e-05\n",
      "  0.00000000e+00 5.00128315e-01]\n",
      " [1.38545954e-01 1.37174211e-03 2.78920897e-02 3.20073160e-03\n",
      "  1.09739369e-02 2.28623686e-03 2.65203476e-02 4.57247372e-04\n",
      "  4.11522634e-03 3.65797897e-03 2.83493370e-02 2.52400549e-01\n",
      "  0.00000000e+00 5.00228624e-01]\n",
      " [8.95317405e-02 3.44872689e-02 1.21708232e-01 9.12321242e-02\n",
      "  4.97471224e-02 2.12831357e-01 1.41022846e-01 1.65678409e-02\n",
      "  1.59530869e-01 3.74520405e-02 4.53435647e-02 5.44994768e-04\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "column_names = [s.name for s in basic_model.states]\n",
    "#order_index = [column_names.index(c) for c in column_order]\n",
    "\n",
    "# re-order the rows/columns to match the specified column order\n",
    "transitions = basic_model.dense_transition_matrix()\n",
    "\n",
    "print(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Traceback (most recent call last):',\n",
       " '  File \"C:\\\\Users\\\\Amith M\\\\anaconda3\\\\Scripts\\\\jupyter-nbconvert-script.py\", line 10, in <module>',\n",
       " '    sys.exit(main())',\n",
       " '  File \"C:\\\\Users\\\\Amith M\\\\anaconda3\\\\lib\\\\site-packages\\\\jupyter_core\\\\application.py\", line 264, in launch_instance',\n",
       " '    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)',\n",
       " '  File \"C:\\\\Users\\\\Amith M\\\\anaconda3\\\\lib\\\\site-packages\\\\traitlets\\\\config\\\\application.py\", line 846, in launch_instance',\n",
       " '    app.start()',\n",
       " '  File \"C:\\\\Users\\\\Amith M\\\\anaconda3\\\\lib\\\\site-packages\\\\nbconvert\\\\nbconvertapp.py\", line 369, in start',\n",
       " '    self.convert_notebooks()',\n",
       " '  File \"C:\\\\Users\\\\Amith M\\\\anaconda3\\\\lib\\\\site-packages\\\\nbconvert\\\\nbconvertapp.py\", line 529, in convert_notebooks',\n",
       " '    raise ValueError(',\n",
       " \"ValueError: Please specify an output format with '--to <format>'.\",\n",
       " \"The following formats are available: ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides', 'webpdf']\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!jupyter nbconvert *.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Amith\n",
      "[nltk_data]     M\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS'),\n",
       " ('any', 'DTI'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "\n",
    "nltk.download('brown')\n",
    "training_corpus = nltk.corpus.brown\n",
    "training_corpus.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_corpus.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 45872\n",
      "Test samples: 11468\n"
     ]
    }
   ],
   "source": [
    "corpus = list(nltk.corpus.brown.tagged_sents())\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(corpus)\n",
    "\n",
    "split = int(0.8 * len(corpus))\n",
    "\n",
    "train = corpus[:split]\n",
    "test = corpus[split:]\n",
    "\n",
    "print('Train samples:', len(train))\n",
    "print('Test samples:', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('He', 'PPS'),\n",
       "  ('let', 'VBD'),\n",
       "  ('her', 'PPO'),\n",
       "  ('tell', 'VB'),\n",
       "  ('him', 'PPO'),\n",
       "  ('all', 'ABN'),\n",
       "  ('about', 'IN'),\n",
       "  ('the', 'AT'),\n",
       "  ('church', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('China', 'NP'),\n",
       "  ('never', 'RB'),\n",
       "  ('tried', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('integrate', 'VB'),\n",
       "  ('Tibet', 'NP'),\n",
       "  ('by', 'IN'),\n",
       "  ('extirpating', 'VBG'),\n",
       "  ('the', 'AT'),\n",
       "  (\"people's\", 'NNS$'),\n",
       "  ('religion', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('institutions', 'NNS'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_data(sequences):\n",
    "    x = []\n",
    "    y = []\n",
    "    w = set()\n",
    "    t = set()\n",
    "    for sequence in sequences:\n",
    "        sequence_x = []\n",
    "        sequence_y = []\n",
    "        for word, tag in sequence:\n",
    "            sequence_x.append(word)\n",
    "            sequence_y.append(tag)\n",
    "            w.add(word)\n",
    "            t.add(tag)\n",
    "        x.append(sequence_x)\n",
    "        y.append(sequence_y)\n",
    "    return x, y, w, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, train_words, train_tagset = rearrange_data(train)\n",
    "test_x, test_y, test_words, test_tagset = rearrange_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'let', 'her', 'tell', 'him', 'all', 'about', 'the', 'church', '.']\n",
      "['PPS', 'VBD', 'PPO', 'VB', 'PPO', 'ABN', 'IN', 'AT', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train words: 50630 number of tags: 450\n",
      "Unknown test words: 5427 Unknown test tags 22\n"
     ]
    }
   ],
   "source": [
    "print('Train words:', len(train_words), 'number of tags:', len(train_tagset))\n",
    "print('Unknown test words:', len(test_words.difference(train_words)), 'Unknown test tags', len(test_tagset.difference(train_tagset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtags(tag_prefix, tagged_text):\n",
    "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text if tag.startswith(tag_prefix))\n",
    "    return dict((tag, cfd[tag].keys()) for tag in cfd.conditions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN ['investigation', 'primary', 'election', 'evidence', 'place']\n",
      "NN$ [\"ordinary's\", \"court's\", \"mayor's\", \"wife's\", \"governor's\"]\n",
      "NN$-HL [\"Golf's\", \"Navy's\"]\n",
      "NN$-TL [\"Department's\", \"Commissioner's\", \"President's\", \"Party's\", \"Mayor's\"]\n",
      "NN-HL ['Merger', 'jail', 'Construction', 'fund', 'sp.']\n",
      "NN-NC ['ova', 'eva', 'aya']\n",
      "NN-TL ['County', 'Jury', 'City', 'Committee', 'Court']\n",
      "NN-TL-HL ['Mayor', 'Commissioner', 'City', 'Oak', 'Grove']\n",
      "NNS ['irregularities', 'presentments', 'thanks', 'reports', 'voters']\n",
      "NNS$ [\"taxpayers'\", \"children's\", \"members'\", \"women's\", \"years'\"]\n",
      "NNS$-HL [\"Dealers'\", \"Idols'\"]\n",
      "NNS$-TL [\"States'\", \"Women's\", \"Princes'\", \"Bombers'\", \"Falcons'\"]\n",
      "NNS-HL ['Wards', 'deputies', 'bonds', 'aspects', 'Decisions']\n",
      "NNS-TL ['Police', 'Roads', 'Legislatures', 'Bankers', 'Reps.']\n",
      "NNS-TL-HL ['Nations']\n"
     ]
    }
   ],
   "source": [
    "tagdict = findtags('NN', nltk.corpus.brown.tagged_words(categories='news'))\n",
    "for tag in sorted(tagdict):\n",
    "    print (tag, list(tagdict[tag])[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags = set(train_tagset)\n",
    "len(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FW-PPO', 'VBG+TO', 'NNS-NC', 'TO-NC', 'FW-IN', 'PPL-NC', 'PP$$', 'VBZ-HL', '.-TL', 'NNS$-NC', 'FW-NN-TL', 'NN+HVZ', 'PN', 'JJR-TL', 'BEZ*', 'PPSS-HL', 'JJR+CS', 'VBN+TO', 'FW-IN+AT', 'RB-TL', '.-HL', 'PP$', 'HVG', ',-HL', 'PPS', 'WDT-NC', 'WDT+BEZ-TL', 'FW-DT+BEZ', 'MD+TO', 'UH', 'VB+IN', 'JJT', 'RB+BEZ-HL', 'VBD-HL', 'EX+MD', 'PPS+BEZ-NC', 'WPS-TL', 'EX-NC', 'VB+AT', 'CD-TL', 'DTS+BEZ', 'PPSS+BER-TL', 'VBG-HL', 'BED*', 'WRB-HL', 'CD', 'NP-HL', 'HV*', 'PPSS+BER-N', 'PPSS+MD', 'AT-TL', 'OD-NC', 'PPSS', 'AP$', 'NP+BEZ', 'DTI', 'FW-VBG', 'JJT-HL', 'NNS-TL-HL', 'HVZ-TL', 'UH-HL', 'WRB+MD', 'EX+BEZ', 'DT+BEZ-NC', 'RP', 'HVZ*', 'BER*-NC', 'DTI-TL', 'TO+VB', 'FW-PPSS', 'VBN-HL', 'WRB+BEZ', 'VB+VB-NC', 'MD*', 'RB$', 'PPSS+BER', 'PN+HVD', 'NR-NC', 'FW-QL', 'AP-TL', 'NR-TL-HL', 'NP$-HL', 'AP', 'DOZ-HL', 'CS-HL', 'FW-VB', 'DOD-NC', ')-HL', 'ABN', 'VBN-NC', 'RB+CS', 'FW-NP', 'BE-HL', 'FW-NN-NC', 'FW-WDT', 'CS', 'CC-TL', 'FW-NN$', 'ABL', 'WQL', 'FW-NNS-NC', 'VBZ', 'NN$-HL', 'NPS-TL', 'VBG-TL', 'PPSS+BEM', 'HVZ-NC', 'DO-HL', 'DO', 'HVD', 'PN+MD', 'NP+HVZ-NC', 'OD-HL', 'ABN-HL', 'BEDZ', '(', 'BEM*', 'WDT+DO+PPS', 'JJR-HL', 'FW-PPL', 'JJR-NC', 'AP+AP-NC', 'FW-*', 'FW-DT', 'FW-AT-HL', 'WRB-NC', 'NN+BEZ', 'FW-VBN', 'TO-HL', 'JJ-TL-NC', 'CC-TL-HL', 'WDT+BEZ-HL', 'NN+NN-NC', 'DO-TL', 'DOD', 'DTS-HL', 'PPSS-TL', 'FW-BER', 'JJ-TL-HL', 'VB+PPO', 'FW-UH-TL', 'WPS+HVD', 'HVN', 'VBN-TL-NC', 'FW-IN+NP-TL', 'CD-HL', 'FW-NN', 'WDT+DOD', 'FW-AT+NN-TL', 'RBT', 'PPO-TL', 'IN-HL', 'NNS$-TL', 'FW-VBG-TL', 'MD-HL', 'PPSS+HV-TL', 'RBR-NC', 'FW-RB', 'VB-HL', 'BEN-TL', 'JJR', 'JJS-TL', 'WRB-TL', ',', 'FW-PP$', 'BER-NC', 'QL', 'FW-RB+CC', 'PPS+HVZ', 'NPS', 'WDT+HVZ', 'FW-VB-NC', ',-NC', 'WRB+BER', 'FW-NPS-TL', 'PPS+MD', '*', 'RB-NC', 'DT+MD', 'PP$-NC', 'FW-NNS-TL', 'NPS$-TL', 'BEZ-HL', 'FW-AT-TL', 'JJ-TL', \"'\", 'DOZ*', 'FW-NP-TL', 'BED', 'QL-HL', 'HV-HL', ':-TL', 'PP$-HL', 'VBD-TL', 'WPS', 'UH-TL', 'VB', 'FW-*-TL', 'JJS-HL', 'NN-NC', 'WRB+DOD*', 'BER', 'IN-TL-HL', 'DO*', 'NP$-TL', 'NP+MD', 'HVD*', 'DTX', 'NRS', 'BEDZ-NC', 'FW-CC-TL', 'DT$', 'RB+BEZ', 'PPSS+HVD', 'PPSS+VB', '(-HL', 'FW-JJ-TL', 'NR-HL', 'FW-NR', 'NP-TL', 'MD-TL', 'UH-NC', 'NR$-TL', 'NNS$-HL', 'DT', 'DO-NC', 'FW-CD', 'JJ', 'FW-HV', 'PPL-TL', 'CS-NC', 'AT-TL-HL', 'NR$', 'VBD', 'JJS', 'HV', 'FW-IN+AT-TL', 'FW-NN-TL-NC', 'ABN-TL', 'TO', 'VB-NC', 'NP+BEZ-NC', 'RP+IN', 'HV-NC', 'CD-TL-HL', 'VBD-NC', 'NN-TL-HL', 'WPO', 'AP-NC', '``', \"''\", 'CC-HL', 'FW-WPS', 'FW-AT', 'PPSS-NC', 'FW-JJ', 'JJ-HL', 'FW-WPO', 'PPL-HL', 'PPSS+BER-NC', 'NN+MD', 'NN+HVD-TL', 'RB-HL', 'FW-JJR', 'FW-IN+AT-T', 'FW-PPO+IN', 'VBG', 'VBN', 'IN+PPO', 'FW-IN-TL', 'HV-TL', 'PN-TL', 'IN-NC', 'FW-PPSS+HV', 'PPS-HL', 'DT-NC', 'FW-JJ-NC', 'VB+JJ-NC', 'PPO-HL', 'FW-UH', 'JJ$-TL', 'DOZ-TL', ',-TL', 'WPS+BEZ-NC', 'CC-NC', 'BE-TL', 'DTS', 'WP$', 'RB+BEZ-NC', 'WPS+MD', 'NN-TL', 'WPS+BEZ', 'PPLS', 'QL-TL', 'WRB+DOD', 'PPSS+BEZ', ':', 'FW-OD-NC', 'VBZ-TL', 'AT-NC', ')', 'IN+IN', 'FW-TO+VB', 'BEM', 'BEG', 'MD', 'FW-VBZ', 'WPO-NC', 'NN-TL-NC', 'CD$', 'FW-CC', 'PN-HL', 'NNS-HL', 'CS-TL', 'FW-PPL+VBZ', 'WQL-TL', 'PN+HVZ', 'NP', 'WRB+BEZ-TL', 'FW-CS', 'BEDZ-HL', 'WDT', 'MD+HV', '---HL', 'EX+HVD', 'FW-DTS', 'WDT+BEZ-NC', 'DOD*', 'DO+PPSS', '*-HL', '*-NC', 'JJT-TL', 'NR-TL', 'EX+HVZ', 'PPS+HVD', 'BER-TL', 'EX', 'NP$', 'JJ+JJ-NC', 'BEZ-NC', 'VBN-TL-HL', 'NPS-NC', 'VBN-TL', 'ABX', 'PPSS+HV', 'RP-HL', 'DT+BEZ', 'BER*', 'PPL', 'WRB', 'DO*-HL', 'PPSS+MD-NC', 'BEM-NC', 'VB+TO', 'AT-HL', 'WPO-TL', ':-HL', 'FW-PP$-TL', 'VB-TL', 'FW-VBD', 'WDT-HL', 'NNS+MD', 'OD', 'NNS-TL-NC', 'QLP', 'FW-NPS', 'IN', 'VBG-NC', 'FW-NR-TL', 'FW-VB-TL', 'NNS$-TL-HL', 'NNS-TL', 'DT-TL', 'FW-PP$-NC', 'FW-RB-TL', 'NNS$', 'RBR', 'PPS-NC', 'WPS+HVZ', 'NIL', 'FW-IN+NN', 'RN', 'TO-TL', '*-TL', 'HV+TO', 'PN$', 'FW-PPS', 'RP-TL', 'BEZ-TL', 'AP-HL', 'FW-VBD-TL', 'DOZ*-TL', 'BE', 'FW-BEZ', 'CD-NC', 'FW-BE', 'WDT+BEZ', 'PPO', 'JJ-NC', 'RP-NC', 'PP$-TL', 'FW-CD-TL', 'BEDZ*', 'MD-NC', 'NN+BEZ-TL', 'NR', 'DOD*-TL', 'VB+RP', 'FW-IN+NN-TL', 'FW-OD-TL', 'NN$', '--', 'PPS+BEZ-HL', 'NR+MD', 'OD-TL', 'BER-HL', 'PPS+BEZ', '.', 'NPS$-HL', 'DOZ', 'BEZ', 'BEN', 'NN$-TL', '.-NC', 'DT-HL', 'NPS$', 'PN-NC', 'NPS-HL', 'NP-TL-HL', 'FW-NNS', 'NP-NC', 'AT', 'PPS-TL', 'IN-TL', 'NN-HL', 'NN+HVZ-TL', 'CC', 'VBZ-NC', 'WPS-HL', 'PN+BEZ', 'WPS+BEZ-TL', 'HVZ', 'BED-NC', 'NP+HVZ', 'FW-NN$-TL', 'RB', 'NNS', 'PPO-NC', 'NN', 'DTI-HL'}\n"
     ]
    }
   ],
   "source": [
    "print(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tags = set(test_tagset)\n",
    "len(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VBG+TO', 'NNS-NC', 'TO-NC', 'FW-IN', 'PP$$', 'VBZ-HL', 'FW-NN-TL', 'PN', 'JJR-TL', 'BEZ*', 'PPSS-HL', 'RB-TL', '.-HL', 'PP$', 'HVG', ',-HL', 'PPS', 'WDT-NC', 'FW-DT+BEZ', 'MD+TO', 'WRB+DOZ', 'UH', 'VB+IN', 'JJT', 'VBD-HL', 'WPS-TL', 'CD-TL', 'DTS+BEZ', 'VBG-HL', 'BED*', 'WRB-HL', 'CD', 'NP-HL', 'HV*', 'NN', 'PPSS+MD', 'AT-TL', 'PPSS', 'AP$', 'WRB+DO', 'NP+BEZ', 'DTI', 'FW-VBG', 'JJT-HL', 'HVZ-TL', 'NNS-TL-HL', 'EX+BEZ', 'RP', 'HVZ*', 'VBN-HL', 'WRB+BEZ', 'MD*', 'PPSS+BER', 'AP-TL', 'NRS-TL', 'NP$-HL', 'AP', 'CS-HL', 'DOZ-HL', 'FW-VB', ')-HL', 'ABN', 'VBN-NC', 'RB+CS', 'FW-NP', 'BE-HL', 'FW-NN-NC', 'FW-WDT', 'CS', 'CC-TL', 'FW-NN$', 'WQL', 'ABL', 'FW-AT+NP-TL', 'VBZ', 'NN$-HL', 'NPS-TL', 'VBG-TL', 'RBR+CS', 'PPSS+BEM', 'DO', 'HVD', 'PN+MD', 'OD-HL', 'BEDZ', 'QL-NC', '(', 'BEM*', 'JJR-HL', 'FW-PPL', 'FW-UH-NC', 'MD+PPSS', 'FW-*', 'NN+BEZ', 'FW-VBN', 'TO-HL', 'WPS-NC', 'ABN-NC', 'DO-TL', 'DOD', 'PPSS-TL', 'JJ-TL-HL', 'VB+PPO', 'WPS+HVD', 'HVN', 'VBN-TL-NC', 'CD-HL', 'FW-NN', 'FW-AT+NN-TL', 'RBT', 'PPO-TL', 'IN-HL', 'NNS$-TL', 'MD-HL', 'FW-RB', 'VB-HL', 'JJR', 'JJS-TL', 'WRB-TL', ',', 'BER-NC', 'QL', 'PPS+HVZ', 'NPS', ',-NC', 'PPS+MD', '*', 'RB-NC', 'DT+MD', 'PP$-NC', 'FW-NNS-TL', 'NPS$-TL', 'BEZ-HL', 'FW-AT-TL', 'JJ-TL', \"'\", 'DOZ*', 'FW-NP-TL', 'BED', ':-TL', 'PP$-HL', 'WPS', 'UH-TL', 'VB', 'NN-NC', 'BER', 'DO*', 'NP$-TL', 'NP+MD', 'PPSS+BEZ*', 'DTX', 'HVD*', 'NRS', 'BEDZ-NC', 'FW-CC-TL', 'DT$', 'RB+BEZ', 'PPSS+HVD', 'FW-JJ-TL', '(-HL', 'NR-HL', 'NP-TL', 'UH-NC', 'NR$-TL', 'NNS$-HL', 'DT', 'FW-CD', 'JJ', 'CS-NC', 'NR$', 'VBD', 'JJS', 'HV', 'ABN-TL', 'TO', 'VB-NC', 'HV-NC', 'CD-TL-HL', 'VBD-NC', 'NN-TL-HL', 'WPO', 'AP-NC', '``', \"''\", 'CC-HL', 'FW-AT', 'PPSS-NC', 'FW-JJ', 'JJ-HL', 'RB-HL', 'FW-PPO+IN', 'VBG', 'VBN', 'FW-IN-TL', 'PN-TL', 'IN-NC', 'PPS-HL', 'DT-NC', 'PPO-HL', 'FW-UH', 'DOZ-TL', ',-TL', 'DTS', 'WP$', 'WPS+MD', 'NN-TL', 'EX-HL', 'WPS+BEZ', 'PPLS', 'QL-TL', 'WRB+DOD', ':', 'VBZ-TL', 'AT-NC', ')', 'NN+IN', 'BEM', 'BEG', 'MD', 'MD*-HL', 'CD$', 'FW-CC', 'PN-HL', 'NNS-HL', 'WQL-TL', 'PN+HVZ', 'NP', 'WRB+BEZ-TL', 'WDT', 'MD+HV', '---HL', 'WDT+BEZ-NC', 'DOD*', '*-HL', 'JJT-TL', 'NR-TL', 'PPS+HVD', 'BER-TL', 'EX', 'NP$', 'VBN-TL-HL', 'VBN-TL', 'ABX', 'PPSS+HV', 'FW-PN', 'RP-HL', 'DT+BEZ', 'BER*', 'PPL', 'WRB', 'AT-HL', 'WPO-TL', 'VB-TL', ':-HL', 'WDT-HL', 'WDT+BER+PP', 'OD', 'NNS-TL-NC', 'QLP', 'IN', 'VBG-NC', 'FW-NR-TL', 'NNS-TL', 'DT-TL', 'NNS$', 'RBR', 'PPS-NC', 'NIL', 'JJT-NC', 'TO-TL', 'PN$', 'RP-TL', 'BEZ-TL', 'AP-HL', 'BE', 'WRB+IN', 'WDT+BEZ', 'PPO', 'JJ-NC', 'PP$-TL', 'BEDZ*', 'NR', '--', 'NN$', 'WDT+BER', 'OD-TL', 'HVG-HL', 'BER-HL', 'PPS+BEZ', '.', 'DOZ', 'BEZ', 'BEN', 'NN$-TL', 'DT-HL', '.-NC', 'FW-JJT', 'NPS$', 'NPS-HL', 'NP-TL-HL', 'FW-NNS', 'AT', 'PPS-TL', 'HVD-HL', 'IN-TL', 'NN-HL', 'CC', 'PN+BEZ', 'HVZ', 'BED-NC', 'NP+HVZ', 'FW-NN$-TL', 'RB', 'NNS', 'HV+TO', 'DTI-HL'}\n"
     ]
    }
   ],
   "source": [
    "print(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtags = nltk.ConditionalFreqDist(nltk.corpus.brown.tagged_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('JJ', 47), ('VB', 15), ('VB-HL', 1), ('RB', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(wordtags[\"clean\"].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that \t 12 ['CS', 'DT', 'WPS', 'WPO', 'QL', 'DT-NC', 'WPS-NC', 'CS-NC', 'WPS-HL', 'CS-HL', 'NIL', 'WPO-NC']\n",
      "A \t 11 ['AT', 'AT-TL', 'NN', 'NP', 'AT-HL', 'NP-HL', 'NP-TL', 'AT-TL-HL', 'NN-TL', 'FW-IN', 'AT-NC']\n",
      "in \t 10 ['IN', 'RP', 'IN-HL', 'IN-TL', 'NIL', 'IN-NC', 'RP-HL', 'RP-NC', 'NN', 'FW-IN']\n",
      "to \t 10 ['TO', 'IN', 'IN-HL', 'TO-HL', 'TO-NC', 'IN-TL', 'IN-NC', 'NIL', 'NPS', 'QL']\n",
      ": \t 9 [':', ':-HL', '.', ':-TL', 'IN', '.-HL', 'NP', ',', 'NIL']\n",
      "a \t 8 ['AT', 'AT-HL', 'AT-NC', 'FW-IN', 'NIL', 'NN', 'FW-IN-TL', 'AT-TL']\n",
      "for \t 7 ['IN', 'CS', 'IN-TL', 'IN-HL', 'RB', 'NN', 'IN-NC']\n",
      "it \t 7 ['PPS', 'PPO', 'PPS-HL', 'PPS-NC', 'PPO-HL', 'PPO-NC', 'UH']\n",
      "well \t 7 ['RB', 'QL', 'UH', 'NN', 'JJ', 'VB', 'QL-HL']\n",
      "as \t 7 ['CS', 'QL', 'IN', 'CS-HL', 'RB', 'CS-TL', 'NIL']\n"
     ]
    }
   ],
   "source": [
    "wtlist = sorted(wordtags.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "for word, freqs in wtlist[:10]:\n",
    "    print(word, \"\\t\", len(freqs), list(freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts_brown(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n",
    "    list that counts the number of occurrences of pair in the sequences list. The input\n",
    "    should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n",
    "    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n",
    "    \"\"\"\n",
    "    tag_bigrams = {}\n",
    "\n",
    "    for sequence in sequences:\n",
    "        \n",
    "        l = len(sequence)\n",
    "        \n",
    "        for index in range(l-1):\n",
    "            \n",
    "            a,b = sequence[index:index+1], sequence[index+1:index+2]\n",
    "            \n",
    "            if (a[0],b[0]) in tag_bigrams.keys():\n",
    "                tag_bigrams[(a[0],b[0])] += 1\n",
    "            else:\n",
    "                tag_bigrams[(a[0],b[0])] = 1\n",
    "                \n",
    "    return tag_bigrams     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_counts = pair_counts(train_y, train_x)\n",
    "tag_unigrams = unigram_counts(train_y)\n",
    "tag_bigrams = bigram_counts_brown(train_y)\n",
    "tag_starts = starting_counts(train_y)\n",
    "tag_ends = ending_counts(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges Brown model: 7818\n"
     ]
    }
   ],
   "source": [
    "brown_model = HiddenMarkovModel(name=\"brown-hmm-tagger\")\n",
    "\n",
    "states = dict()\n",
    "for tag, words in emission_counts.items():\n",
    "    #n = tag_unigrams[tag]\n",
    "    #assert n == sum(words.values())\n",
    "    probs = {w:c / tag_unigrams[tag] for w, c in words.items()}\n",
    "    emissions = DiscreteDistribution(probs)\n",
    "    state = State(emissions, name=tag)\n",
    "    brown_model.add_states(state)\n",
    "    states[tag] = state\n",
    "\n",
    "n = sum(tag_starts.values())\n",
    "for tag, counts in tag_starts.items():\n",
    "    brown_model.add_transition(brown_model.start, states[tag], counts / n)\n",
    "\n",
    "for (tag1, tag2), counts in tag_bigrams.items():\n",
    "    brown_model.add_transition(states[tag1], states[tag2], counts / tag_unigrams[tag1])\n",
    "\n",
    "for tag, counts in tag_ends.items():\n",
    "    brown_model.add_transition(states[tag], brown_model.end, counts / tag_unigrams[tag])\n",
    "\n",
    "brown_model.bake()\n",
    "\n",
    "print('Edges Brown model:', brown_model.edge_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown_brown(sequence, vocabulary):\n",
    "    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n",
    "    by the literal string value 'nan'. Pomegranate will ignore these values\n",
    "    during computation.\n",
    "    \"\"\"\n",
    "    return [w if w in vocabulary else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding_brown(X, model, vocabulary):\n",
    "    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n",
    "    _, state_path = model.viterbi(replace_unknown_brown(X, vocabulary))\n",
    "    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions\n",
    "\n",
    "def accuracy_brown(X, Y, model, vocabulary):\n",
    "    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\n",
    "    in the input X and comparing the prediction with the true labels in Y.\n",
    "    \n",
    "    The X should be an array whose first dimension is the number of sentences to test,\n",
    "    and each element of the array should be an iterable of the words in the sequence.\n",
    "    The arrays X and Y should have the exact same shape.\n",
    "    \n",
    "    X = [(\"See\", \"Spot\", \"run\"), (\"Run\", \"Spot\", \"run\", \"fast\"), ...]\n",
    "    Y = [(), (), ...]\n",
    "    \"\"\"\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        \n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding_brown(observations, model, vocabulary)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 97.32%\n",
      "testing accuracy: 92.52%\n"
     ]
    }
   ],
   "source": [
    "training_acc = accuracy_brown(train_x, train_y, brown_model, vocabulary=train_words)\n",
    "print(\"training accuracy: {:.2f}%\".format(100 * training_acc))\n",
    "\n",
    "testing_acc = accuracy_brown(test_x, test_y, brown_model, vocabulary=train_words)\n",
    "print(\"testing accuracy: {:.2f}%\".format(100 * testing_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigrams_test = bigram_counts_brown(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extend transition model including 7552 train bigrams by 458 new bigrams\n",
      "Edges Brown model with Laplace Smoothing: 8276\n"
     ]
    }
   ],
   "source": [
    "Smooth_Brown_model = HiddenMarkovModel(name=\"brown-smooth-hmm-tagger\")\n",
    "\n",
    "states = dict()\n",
    "for tag, words in emission_counts.items():\n",
    "    n = tag_unigrams[tag]\n",
    "    assert n == sum(words.values())\n",
    "    probs = {w:c / n for w, c in words.items()}\n",
    "    emissions = DiscreteDistribution(probs)\n",
    "    state = State(emissions, name=tag)\n",
    "    Smooth_Brown_model.add_states(state)\n",
    "    states[tag] = state\n",
    "\n",
    "n = sum(tag_starts.values())\n",
    "for tag, counts in tag_starts.items():\n",
    "    Smooth_Brown_model.add_transition(Smooth_Brown_model.start, states[tag], counts / n)\n",
    "\n",
    "for (tag1, tag2), counts in tag_bigrams.items():\n",
    "    nominator = counts + 1\n",
    "    denominator = tag_unigrams[tag1] + len(train_tagset)\n",
    "    Smooth_Brown_model.add_transition(states[tag1], states[tag2], nominator / denominator)\n",
    "\n",
    "for tag, counts in tag_ends.items():\n",
    "    Smooth_Brown_model.add_transition(states[tag], Smooth_Brown_model.end, counts / tag_unigrams[tag])\n",
    "\n",
    "# NOTE: counts statistics from the test set is not used\n",
    "\n",
    "new_bigrams = 0\n",
    "for (tag1, tag2), counts in tag_bigrams_test.items():\n",
    "    if (tag1, tag2) in tag_bigrams:\n",
    "        continue\n",
    "    if tag1 not in states or tag2 not in states:\n",
    "        continue\n",
    "    denominator = len(train_tagset)\n",
    "    if tag1 in tag_unigrams:\n",
    "        denominator += tag_unigrams[tag1]\n",
    "    Smooth_Brown_model.add_transition(states[tag1], states[tag2], 1 / denominator)\n",
    "    new_bigrams += 1\n",
    "\n",
    "print('Extend transition model including', len(tag_bigrams), 'train bigrams by', new_bigrams, 'new bigrams')\n",
    "    \n",
    "Smooth_Brown_model.bake()\n",
    "\n",
    "print('Edges Brown model with Laplace Smoothing:', Smooth_Brown_model.edge_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 97.30%\n",
      "testing accuracy: 93.86%\n"
     ]
    }
   ],
   "source": [
    "training_acc = accuracy_brown(train_x, train_y, Smooth_Brown_model, vocabulary=train_words)\n",
    "print(\"training accuracy: {:.2f}%\".format(100 * training_acc))\n",
    "\n",
    "testing_acc = accuracy_brown(test_x, test_y, Smooth_Brown_model, vocabulary=train_words)\n",
    "print(\"testing accuracy: {:.2f}%\".format(100 * testing_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence:\n",
      "-----------------\n",
      "['According', 'to', 'state', 'law', 'a', 'slave', 'had', 'to', 'be', 'at', 'least', 'thirty', 'years', 'old', 'before', 'he', 'could', 'be', 'freed', '.']\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['IN', 'IN', 'NN', 'NN', 'AT', 'NN', 'HVD', 'TO', 'BE', 'IN', 'AP', 'CD', 'NNS', 'JJ', 'CS', 'PPS', 'MD', 'BE', 'VBN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "['IN', 'IN', 'NN', 'NN', 'AT', 'NN', 'HVD', 'TO', 'BE', 'IN', 'AP', 'CD', 'NNS', 'JJ', 'CS', 'PPS', 'MD', 'BE', 'VBN', '.']\n",
      "\n",
      "\n",
      "Test sentence:\n",
      "-----------------\n",
      "['With', 'tips', ',', 'the', 'girls', 'average', 'between', '$150', 'and', '$200', 'a', 'week', ',', 'depending', 'on', 'basic', 'salary', '.']\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['IN', 'NNS', ',', 'AT', 'NNS', 'NN', 'IN', 'NNS', 'CC', 'NNS', 'AT', 'NN', ',', 'VBG', 'IN', 'JJ', 'NN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "['IN', 'NNS', ',', 'AT', 'NNS', 'VB', 'IN', 'NNS', 'CC', 'NNS', 'AT', 'NN', ',', 'IN', 'IN', 'JJ', 'NN', '.']\n",
      "\n",
      "\n",
      "Test sentence:\n",
      "-----------------\n",
      "['It', 'was', 'a', 'very', 'tempting', 'offer', '.']\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PPS', 'BEDZ', 'AT', 'QL', 'VBG', 'NN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "['PPS', 'BEDZ', 'AT', 'QL', 'VBG', 'NN', '.']\n",
      "\n",
      "\n",
      "Test sentence:\n",
      "-----------------\n",
      "['He', 'saw', 'the', 'surprise', 'in', 'her', 'face', ',', 'and', 'laughed', 'as', 'though', 'it', 'were', 'the', 'funniest', 'expression', 'he', 'had', 'ever', 'seen', '.']\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PPS', 'VBD', 'AT', 'NN', 'IN', 'PP$', 'NN', ',', 'CC', 'VBD', 'CS', 'CS', 'PPS', 'BED', 'AT', 'JJT', 'NN', 'PPS', 'HVD', 'RB', 'VBN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "['PPS', 'VBD', 'AT', 'NN', 'IN', 'PP$', 'NN', ',', 'CC', 'VBD', 'CS', 'CS', 'PPS', 'BED', 'AT', 'JJT', 'NN', 'PPS', 'HVD', 'RB', 'VBN', '.']\n",
      "\n",
      "\n",
      "Test sentence:\n",
      "-----------------\n",
      "['Add', 'holes', 'in', 'top', ',', 'forming', '``', 'S', \"''\", 'for', 'salt', 'and', '``', 'P', \"''\", 'for', 'pepper', '.']\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['VB', 'NNS', 'IN', 'NN', ',', 'VBG', '``', 'NP', \"''\", 'IN', 'NN', 'CC', '``', 'NP', \"''\", 'IN', 'NN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "['VB', 'NNS', 'IN', 'NN', ',', 'VBG', '``', 'NN', \"''\", 'IN', 'NN', 'CC', '``', 'NN', \"''\", 'IN', 'NN', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence, tag in zip(test_x[:5], test_y[:5]):\n",
    "    print(\"Test sentence:\\n-----------------\")\n",
    "    print(sentence)\n",
    "    print()\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding_brown(sentence, Smooth_Brown_model, vocabulary=train_words))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(tag)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence, tag \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI ate an apple\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest sentence:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sentence)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "for sentence, tag in Test(\"I ate an apple\"):\n",
    "    print(\"Test sentence:\\n-----------------\")\n",
    "    print(sentence)\n",
    "    print()\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding_brown(sentence, Smooth_Brown_model, vocabulary=train_words))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(tag)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
